在这篇文档里，我们将介绍两种分配探索和利用的时间的方式——$\epsilon$-Greedy Policies和exploration fuctions。
# $\epsilon$-Greedy Policies
这个很简单，$\epsilon$的概率探索，$1 - \epsilon$的概率利用
# Exploration Functions
之前我们已经介绍了Q-Learning的方法，我们只需将Q的迭代稍加改动：
$$Q(s, a) \gets (1 - \alpha)Q(s, a) + \alpha(R(s, a, s^\prime) + \gamma \max_{a^\prime}f(s^\prime, a^\prime))$$
在这里，我们只是将$Q(s^\prime, a^\prime)$变成$f(s^\prime, a^\prime)$了而已，函数$f$的一个选择是：
$$f(s, a) = Q(s, a) + \displaystyle \frac{k}{N(s, a)}$$
其中，$k$是定值，$N(s, a)$表示$(s,a )$被探索过的次数。

这样做的目的是：增大了那些较少探索过的状态的价值，使得智能体愿意去更多的尝试没有尝试过的状态。